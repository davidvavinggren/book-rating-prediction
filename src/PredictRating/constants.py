# Tokenizer
MAX_LEN = 512

# Training
DATA_PERCENTAGE = 0.5
TRAIN_SPLIT = 0.8
TRAIN_BATCH_SIZE = 20
TEST_BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-05

# Network
DROPOUT_PROB = 0.2